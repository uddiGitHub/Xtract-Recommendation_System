{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f55fda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98294ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>category_code</th>\n",
       "      <th>update_date</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194060</th>\n",
       "      <td>1506.0457</td>\n",
       "      <td>The Two-envelope Problem: An Informed Choice</td>\n",
       "      <td>The host of a game presents two indistinguis...</td>\n",
       "      <td>Jeffrey Brian Tyler</td>\n",
       "      <td>stat.OT</td>\n",
       "      <td>2015-06-16</td>\n",
       "      <td>the two envelope problem an informed choice</td>\n",
       "      <td>the host of a game presents two indistinguisha...</td>\n",
       "      <td>Statistics – Other Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194061</th>\n",
       "      <td>1904.0433</td>\n",
       "      <td>The Contribution Plot: Decomposition and Graph...</td>\n",
       "      <td>Alzheimer's disease (AD) is a chronic neurod...</td>\n",
       "      <td>JinCheol Choi, Donghuan Lu, Mirza Faisal Beg, ...</td>\n",
       "      <td>stat.OT</td>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>the contribution plot decomposition and graphi...</td>\n",
       "      <td>alzheimer s disease ad is a chronic neurodegen...</td>\n",
       "      <td>Statistics – Other Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194062</th>\n",
       "      <td>2508.09563</td>\n",
       "      <td>Performances and Correlations of Centrality Me...</td>\n",
       "      <td>Numerous centrality measures have been propose...</td>\n",
       "      <td>Yilin Bi, Xinshan Jiao, Tao Zhou</td>\n",
       "      <td>stat.OT</td>\n",
       "      <td>2025-08-14</td>\n",
       "      <td>performances and correlations of centrality me...</td>\n",
       "      <td>numerous centrality measures have been propose...</td>\n",
       "      <td>Statistics – Other Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194063</th>\n",
       "      <td>1410.8868</td>\n",
       "      <td>Precinct Size Matters - The Large Precinct Bia...</td>\n",
       "      <td>Examination of precinct level data in US pre...</td>\n",
       "      <td>Glenn Webb</td>\n",
       "      <td>stat.OT</td>\n",
       "      <td>2014-11-03</td>\n",
       "      <td>precinct size matters the large precinct bias ...</td>\n",
       "      <td>examination of precinct level data in us presi...</td>\n",
       "      <td>Statistics – Other Statistics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194064</th>\n",
       "      <td>2202.1248</td>\n",
       "      <td>Revisiting the secondary climate attributes fo...</td>\n",
       "      <td>Environmental conditions in various regions ...</td>\n",
       "      <td>Tao Liao, Paul Kepley, Indraneel Kumar, Samuel...</td>\n",
       "      <td>stat.OT</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>revisiting the secondary climate attributes fo...</td>\n",
       "      <td>environmental conditions in various regions ca...</td>\n",
       "      <td>Statistics – Other Statistics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                              title  \\\n",
       "194060   1506.0457       The Two-envelope Problem: An Informed Choice   \n",
       "194061   1904.0433  The Contribution Plot: Decomposition and Graph...   \n",
       "194062  2508.09563  Performances and Correlations of Centrality Me...   \n",
       "194063   1410.8868  Precinct Size Matters - The Large Precinct Bia...   \n",
       "194064   2202.1248  Revisiting the secondary climate attributes fo...   \n",
       "\n",
       "                                                 abstract  \\\n",
       "194060    The host of a game presents two indistinguis...   \n",
       "194061    Alzheimer's disease (AD) is a chronic neurod...   \n",
       "194062  Numerous centrality measures have been propose...   \n",
       "194063    Examination of precinct level data in US pre...   \n",
       "194064    Environmental conditions in various regions ...   \n",
       "\n",
       "                                                  authors category_code  \\\n",
       "194060                                Jeffrey Brian Tyler       stat.OT   \n",
       "194061  JinCheol Choi, Donghuan Lu, Mirza Faisal Beg, ...       stat.OT   \n",
       "194062                   Yilin Bi, Xinshan Jiao, Tao Zhou       stat.OT   \n",
       "194063                                         Glenn Webb       stat.OT   \n",
       "194064  Tao Liao, Paul Kepley, Indraneel Kumar, Samuel...       stat.OT   \n",
       "\n",
       "       update_date                                        clean_title  \\\n",
       "194060  2015-06-16        the two envelope problem an informed choice   \n",
       "194061  2019-04-10  the contribution plot decomposition and graphi...   \n",
       "194062  2025-08-14  performances and correlations of centrality me...   \n",
       "194063  2014-11-03  precinct size matters the large precinct bias ...   \n",
       "194064  2022-02-28  revisiting the secondary climate attributes fo...   \n",
       "\n",
       "                                           clean_abstract  \\\n",
       "194060  the host of a game presents two indistinguisha...   \n",
       "194061  alzheimer s disease ad is a chronic neurodegen...   \n",
       "194062  numerous centrality measures have been propose...   \n",
       "194063  examination of precinct level data in us presi...   \n",
       "194064  environmental conditions in various regions ca...   \n",
       "\n",
       "                             category  \n",
       "194060  Statistics – Other Statistics  \n",
       "194061  Statistics – Other Statistics  \n",
       "194062  Statistics – Other Statistics  \n",
       "194063  Statistics – Other Statistics  \n",
       "194064  Statistics – Other Statistics  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../xtract-api/DataSet/arxiv_processed.csv\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eaa09d",
   "metadata": {},
   "source": [
    "### Text Preprocessing (combine = Title + abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "299bfb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e4f8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fa19d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9ec6e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced text preprocessing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "data['combined_text'] = (data['clean_title'] + ' ' + data['clean_abstract']).apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f856cf4",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09dd1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.8\n",
    ")\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bd122da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category encoding (if you want to use categories)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "category_encoded = label_encoder.fit_transform(data['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8a6dd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (194065, 10001)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# Combine TF-IDF with category features\n",
    "category_matrix = np.array(category_encoded).reshape(-1, 1)\n",
    "feature_matrix = hstack([tfidf_matrix, category_matrix])\n",
    "\n",
    "print(f\"Feature matrix shape: {feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8a3046",
   "metadata": {},
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43979303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create mapping between paper IDs and indices\n",
    "paper_id_to_idx = {paper_id: idx for idx, paper_id in enumerate(data['id'])}\n",
    "idx_to_paper_id = {idx: paper_id for paper_id, idx in paper_id_to_idx.items()}\n",
    "\n",
    "# Convert to CSR format for efficient row indexing\n",
    "feature_matrix_csr = feature_matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3faf4e0",
   "metadata": {},
   "source": [
    "### Recommendation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c8923cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_for_index(paper_id, feature_matrix, top_k=10):\n",
    "    \"\"\"Compute similarity for a specific paper using paper_id\"\"\"\n",
    "    # Convert paper_id to the correct type if needed\n",
    "   \n",
    "    \n",
    "    if paper_id not in paper_id_to_idx:\n",
    "        # Try alternative formats\n",
    "        try:\n",
    "            paper_id_float = float(paper_id)\n",
    "            if paper_id_float in paper_id_to_idx:\n",
    "                idx = paper_id_to_idx[paper_id_float]\n",
    "            else:\n",
    "                return None  # Paper not found\n",
    "        except (ValueError, TypeError):\n",
    "            return None  # Paper not found\n",
    "    else:\n",
    "        idx = paper_id_to_idx[paper_id]\n",
    "    \n",
    "    query_vector = feature_matrix[idx]\n",
    "    similarities = cosine_similarity(query_vector, feature_matrix)\n",
    "    return similarities.flatten()\n",
    "\n",
    "def get_recommendations_on_the_fly(paper_id, top_k=10):\n",
    "    \"\"\"Get recommendations by computing similarity on-the-fly\"\"\"\n",
    "    # Convert paper_id to string for consistency\n",
    "    \n",
    "    if paper_id not in paper_id_to_idx:\n",
    "        # Try alternative formats\n",
    "        try:\n",
    "            paper_id_float = float(paper_id)\n",
    "            if paper_id_float not in paper_id_to_idx:\n",
    "                return f\"Paper ID {paper_id} not found. Please check the paper ID.\"\n",
    "            else:\n",
    "                idx = paper_id_to_idx[paper_id_float]\n",
    "        except (ValueError, TypeError):\n",
    "            return f\"Paper ID {paper_id} not found. Please check the paper ID.\"\n",
    "    else:\n",
    "        idx = paper_id_to_idx[paper_id]\n",
    "    \n",
    "    # Compute similarities only for this specific paper\n",
    "    sim_scores = compute_similarity_for_index(paper_id, feature_matrix_csr)\n",
    "    \n",
    "    if sim_scores is None:\n",
    "        return f\"Paper ID {paper_id} not found. Please check the paper ID.\"\n",
    "    \n",
    "    # Create list of (index, score) pairs\n",
    "    sim_scores = list(enumerate(sim_scores))\n",
    "    \n",
    "    # Sort by similarity score (descending)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top_k most similar papers (excluding the paper itself)\n",
    "    sim_scores = sim_scores[1:top_k+1]\n",
    "    \n",
    "    # Extract indices and scores\n",
    "    paper_indices = [i[0] for i in sim_scores]\n",
    "    similarity_scores = [i[1] for i in sim_scores]\n",
    "    \n",
    "    # Return recommendations\n",
    "    recommendations = data.iloc[paper_indices][['id', 'title', 'authors', 'category']].copy()\n",
    "    recommendations['similarity_score'] = similarity_scores\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf89708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On-the-fly Recommendations for paper 704.0033:\n",
      "                      id                                              title  \\\n",
      "103673  cond-mat/0412039          Reply to Bernevig, Giuliano, and Laughlin   \n",
      "148072        2409.15709                                     $R(5,5)\\le 46$   \n",
      "148957        1703.08768                                    $R(5,5) \\le 48$   \n",
      "189731           810.542  The effect of dipole-dipole interaction for tw...   \n",
      "191129         1103.1442                  Two-photon dipole-dipole blockade   \n",
      "\n",
      "                                                  authors  \\\n",
      "103673                  Martin Greiter and Dirk Schuricht   \n",
      "148072            Vigleik Angeltveit and Brendan D. McKay   \n",
      "148957            Vigleik Angeltveit and Brendan D. McKay   \n",
      "189731                  Yang Li, Jiang Zhou, and Hong Guo   \n",
      "191129  Khulud Almutairi, Ryszard Tanas, and Zbigniew ...   \n",
      "\n",
      "                                                category  similarity_score  \n",
      "103673  Condensed Matter – Strongly Correlated Electrons               1.0  \n",
      "148072                       Mathematics – Combinatorics               1.0  \n",
      "148957                       Mathematics – Combinatorics               1.0  \n",
      "189731                                   Quantum Physics               1.0  \n",
      "191129                                   Quantum Physics               1.0  \n"
     ]
    }
   ],
   "source": [
    "# Test the recommendation systems\n",
    "print(\"\\nOn-the-fly Recommendations for paper 704.0033:\")\n",
    "on_the_fly_recommendations = get_recommendations_on_the_fly('704.0033', top_k=5)\n",
    "print(on_the_fly_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1cab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae25941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
